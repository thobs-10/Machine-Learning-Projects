# -*- coding: utf-8 -*-
"""Stroke Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8AlwiIH6boGicEa1c5knp1kNyBkVIac

# Stroke Prediction Project

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

plt.rcParams['figure.figsize'] = (20,20)
#figure(num=None, figsize=(8, 6), dpi=800, facecolor='w', edgecolor='k')

#!pip install pandas-profiling

#!pip install dtale

#from pandas_profiling import ProfileReport

#import dtale

"""### Import Dataset"""

from google.colab import files
files.upload()

# create a kaggle directory
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# permission for json files to act
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d fedesoriano/stroke-prediction-dataset

!pip install zip_files

from zipfile import ZipFile
# open the zipped file
file_name="stroke-prediction-dataset.zip"
with ZipFile(file_name,'r') as zip:
      zip.extractall()
      print('Done')





df = pd.read_csv(r'/content/healthcare-dataset-stroke-data.csv')
df.head()



"""### Explolatory Data Analysis"""

# get info about the data
df.info()

# describe the dataset
df.describe().T

# dataset shape
df.shape

"""### Univariate Analysis

### Categorical Columns

#### Inspecting Gender Column
"""

import seaborn as sns

# distribution for Gender
sns.countplot(x = "gender", data= df)
plt.show()

"""Comments or Findings: 

"""

sns.countplot(x ='gender', hue = "hypertension", data = df)
plt.show()

"""Comments or Findings:
The results above have shown the most gender that has hypertension is female and the most gender that has negative  effect on hypertension is female.
"""

sns.countplot(x ='gender', hue = "heart_disease", data = df)
plt.show()

"""Comments or Findings: The most gender that has negative results for heart disease is females and the gender is mostly affected by heart disease is male."""



"""#### Inspecting Hyptertension Column"""

sns.countplot(x = "hypertension", data= df)
plt.show()

sns.countplot(x ='hypertension', hue = "heart_disease", data = df)
plt.show()

sns.countplot(x ='hypertension', hue = "ever_married", data = df)
plt.show()

"""#### Inspecting  Heart Disease Column"""

sns.countplot(x = "heart_disease", data= df)
plt.show()

sns.countplot(x ='heart_disease', hue = "ever_married", data = df)
plt.show()

# check for null values
df.isnull().sum()



"""### Continuous Columns

#### Inspecting Average Glucose Level
"""

sns.displot(df['avg_glucose_level'])
plt.show()

sns.distplot(df['avg_glucose_level'])
plt.show()

sns.boxplot(df['avg_glucose_level'])
plt.show()

# removee the outliers
def remove_avg_glucose_level_outliers(df):
    # calculate the Quantiles(Q1 and Q3)
    Q1 = df['avg_glucose_level'].quantile(0.25)
    Q3 = df['avg_glucose_level'].quantile(0.75)
    # calclulate the Inter_quatile_range IQR
    IQR = Q3 - Q1
    # calculate the lower limit and upper  limit (LL & UL)
    LL = Q1 - 1.5 * IQR
    UL = Q3 + 1.5 * IQR
    # now filter the column to remove the outliers
    # replace all the values that are less or equal to the LL in the hours per weeek column with the LL
    df.loc[df['avg_glucose_level'] <= LL, 'avg_glucose_level'] = LL
    # do the same for values greater than the UL
    df.loc[df['avg_glucose_level'] >= UL, 'avg_glucose_level'] = UL

remove_avg_glucose_level_outliers(df)

sns.boxplot(df['avg_glucose_level'])

sns.distplot(df['avg_glucose_level'])





"""Comments & findings :

#### Inspecting BMI
"""

sns.displot(df['bmi'])
plt.show()

sns.distplot(df['bmi'])
plt.show()

sns.boxplot(df['bmi'])
plt.show()

# removee the outliers
def remove_bmi_outliers(df):
    # calculate the Quantiles(Q1 and Q3)
    Q1 = df['bmi'].quantile(0.25)
    Q3 = df['bmi'].quantile(0.75)
    # calclulate the Inter_quatile_range IQR
    IQR = Q3 - Q1
    # calculate the lower limit and upper  limit (LL & UL)
    LL = Q1 - 1.5 * IQR
    UL = Q3 + 1.5 * IQR
    # now filter the column to remove the outliers
    # replace all the values that are less or equal to the LL in the hours per weeek column with the LL
    df.loc[df['bmi'] <= LL, 'bmi'] = LL
    # do the same for values greater than the UL
    df.loc[df['bmi'] >= UL, 'bmi'] = UL

remove_bmi_outliers(df)

sns.distplot(df['bmi'])

sns.boxplot(df['bmi'])

"""Comments & findings :"""



"""### Bivariate Analysis

#### Continous  Features
"""

sns.pairplot(df, hue ='gender')
plt.show()

sns.pairplot(df, hue ='hypertension')
plt.show()

sns.pairplot(df, hue ='heart_disease')
plt.show()

"""#### Categorical  Features"""

# gender v/s bmi barplot
sns.barplot(x = 'gender',y = 'bmi',data = df)
plt.show()

sns.barplot(x = 'gender',y = 'bmi', hue = 'hypertension',data = df)
plt.show()

sns.barplot(x = 'gender',y = 'bmi', hue = 'heart_disease',data = df)
plt.show()

"""## Data Preprocessing

## Feature engineering

### label Encoding
"""

df.head()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

"""#### gender feature"""

df['gender'].unique()

gender = le.fit_transform(df['gender'])

df['gender'] = gender

df.head()

"""#### Work Type"""

df['work_type'].unique()

work_type = le.fit_transform(df['work_type'])
df['work_type'] = work_type

df.head()

"""#### Residence"""

df['Residence_type'].unique()

res_type = le.fit_transform(df['Residence_type'])
df['Residence_type'] = res_type

df.head()



"""#### Smoking Type"""

df['smoking_status'].unique()

smoking_type = le.fit_transform(df['smoking_status'])
df['smoking_status'] = smoking_type
df.head()

df.info()





"""#### Marriage Column"""

df.head(3)

df['ever_married'].unique()

married_ever = le.fit_transform(df['ever_married'])
df['ever_married'] = married_ever
df.head()





"""### Missing Values"""

# using heatmap to check null values
sns.heatmap(df.isnull(), yticklabels=False, cbar = False, cmap = 'viridis')

df['bmi'].isnull().sum()

df['avg_glucose_level'].isnull().sum()

# fill the null values for bmi with average value
df['bmi'].fillna(df['bmi'].mean(), inplace = True)

df['bmi'].isnull().sum()

sns.heatmap(df.isnull(), yticklabels=False, cbar = False, cmap = 'viridis')

"""### Outlier Detection"""

# outlier removal
df.plot(kind = 'box',figsize=(20,15))
plt.show()



"""### Correlation Analysis & Feature Relationship"""

plt.figure(figsize=(15, 15))
corr = df.corr()
sns.heatmap(corr, annot=True)

# drop the id column
df.drop('id', axis = 1, inplace = True)

#### Remove The correlated
threshold=0.7

# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

correlation(df.iloc[:,:-1],threshold)

sns.scatterplot(data = df, x ='avg_glucose_level', y = 'bmi', hue = 'heart_disease')

"""### Split the dataset into dependent and independent features"""

X=df.drop('stroke',axis=1)

X.head()

y=df['stroke']

y

"""### Feature Importance"""

# Important feature using ExtraTreesRegressor
from sklearn.ensemble import ExtraTreesRegressor
selection = ExtraTreesRegressor()
selection.fit(X, y)

print(selection.feature_importances_)

plt.figure(figsize=(12, 10))
feature_imp = pd.Series(selection.feature_importances_, index=X.columns)
feature_imp.nlargest(20).plot(kind='barh')
plt.show()

from sklearn.feature_selection import mutual_info_classif

mutual_info=mutual_info_classif(X,y)

mutual_data=pd.Series(mutual_info,index=X.columns)
mutual_data.sort_values(ascending=False)

"""### Split Dataset into Train and Test Set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

X_train.shape

y_train.shape

X_test.shape

y_test.shape

"""### Imbalance Dataset

##### Dealing with Categorical Features
"""

sns.countplot(x = "gender", data= df)
plt.show()

sns.countplot(x = "hypertension", data= df)
plt.show()

df['hypertension'].value_counts()

from collections import Counter

from imblearn.combine import SMOTETomek

print("The number of classes before fit: {}".format( y_train.value_counts()))

#os = SMOTETomek(0.5)
#X_train, y_train = os.fit_resample(X_trXain, y_train)
#print("The number of classes after fit {}".format((( y_train.value_counts()))))

#Importing SMOTE
from imblearn.over_sampling import SMOTE
#Oversampling the data

smote = SMOTE(random_state = 1)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)
#Creating a new Oversampling Data Frame
#df_oversampler = pd.DataFrame(X, columns = ['CreditScore', 'Age'])
#df_oversampler['Exited']
#sns.countplot(df_oversampler['Exited'])



sns.countplot(x = "heart_disease", data= df)
plt.show()



sns.countplot(x = "work_type", data= df)
plt.show()

#!pip install imblearn

#y_train.value_counts()



"""### Dimensionality Reduction"""



X.head()

# drop the residence type column
#X.drop('Residence_type', axis = 1, inplace = True)
X.head()

from sklearn.decomposition import PCA
pca = PCA(n_components = 7)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

print(abs( pca.components_ ))

"""featue : """

X_train.shape

X_test.shape

# number of components
n_pcs= pca.components_.shape[0]
n_pcs

# get the index of the most important feature on EACH component i.e. largest absolute value
# using LIST COMPREHENSION HERE
most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]
most_important

df.iloc[:,0:1]

initial_feature_names = ['gender',	'age',	'hypertension',	'heart_disease',	'ever_married',	'work_type',	'Residence_type'	,'avg_glucose_level'	,'bmi',	'smoking_status']

# get the names
most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]

# using LIST COMPREHENSION HERE AGAIN
dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}

# build the dataframe
df = pd.DataFrame(sorted(dic.items()))

df

"""### Scale Dataset"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X_train

"""#### Save Scaler Object"""

import pickle
import os

from google.colab import drive
drive.mount('/content/drive')

#scaler_path = '/content/drive/MyDrive/scaler.pkl'
#with open(scaler_path,'wb') as scaler_file:
    #pickle.dump(sc,scaler_file)

"""## Training

#### Logistic Regresion
"""

from sklearn.linear_model import LogisticRegression
log_classifier = LogisticRegression(random_state = 0)
log_classifier.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_train_pred_log  = log_classifier.predict(X_train)

accuracy_score(y_train, y_train_pred_log)

X_train

"""##### Evaluate Model"""

print(log_classifier.predict(sc.transform([[169.357,67,36.600000,1,2,1,1]])))

y_pred_log = log_classifier.predict(X_test)

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred_log)
print(cm)
print(classification_report(y_test, y_pred_log))
log_acc = accuracy_score(y_test, y_pred_log)
log_acc

print(classification_report(y_test, y_pred_log))



"""#### KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn_classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
knn_classifier.fit(X_train, y_train)



"""##### Evaluate Model"""

print(knn_classifier.predict(sc.transform([[169.357,67,36.600000,1,2,1,1]])))

"""##### Training Accuracy"""

y_train_pred_knn  = knn_classifier.predict(X_train)

accuracy_score(y_train, y_train_pred_knn)

"""##### Test Accuracy"""

y_pred_knn = knn_classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred_knn)
print(cm)
knn_acc = accuracy_score(y_test, y_pred_knn)
knn_acc

print(classification_report(y_test, y_pred_knn))

"""#### Support Vector Machine(Classifier)"""

from sklearn.svm import SVC
svm_classifier = SVC(kernel = 'linear', random_state = 0)
svm_classifier.fit(X_train, y_train)

print(svm_classifier.predict([[169.357,67,36.600000,1,2,1,1]]))

"""##### Evaluate Model

##### Training Accuracy
"""

y_train_pred_svm  = svm_classifier.predict(X_train)

accuracy_score(y_train, y_train_pred_svm)

"""##### Test Accuracy"""



y_pred_svm = svm_classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred_svm)
print(cm)
svm_acc = accuracy_score(y_test, y_pred_svm)
svm_acc



"""#### SVM(Kernel)"""

from sklearn.svm import SVC
kernel_classifier = SVC(kernel = 'rbf', random_state = 0)
kernel_classifier.fit(X_train, y_train)

print(kernel_classifier.predict([[169.357,67,36.600000,1,2,1,1]]))

"""##### Evaluate Model"""

print(kernel_classifier.predict(sc.transform([[169.357,67,36.600000,1,2,1,1]])))

y_pred_kernel = kernel_classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred_kernel)
print(cm)
kernel_acc = accuracy_score(y_test, y_pred_kernel)
kernel_acc

"""#### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

"""##### Evaluate Model"""

print(nb_classifier.predict(sc.transform([[169.357,67,36.600000,1,2,1,1]])))

y_pred = nb_classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
nb_acc = accuracy_score(y_test, y_pred)
nb_acc

"""#### Decision Tree """

from sklearn.tree import DecisionTreeClassifier
dt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
dt_classifier.fit(X_train, y_train)

"""##### Evaluate Model"""

print(dt_classifier.predict(sc.transform([[169.357,67,36.600000,1,2,1,1]])))

y_pred = dt_classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
dt_acc = accuracy_score(y_test, y_pred)
dt_acc



"""#### Random Forest Tree"""

from sklearn.ensemble import RandomForestClassifier
rf_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
rf_classifier.fit(X_train, y_train)

"""##### Evaluate Model"""

print(rf_classifier.predict(sc.transform([[169.357,67,36.600000,1,2,1,1]])))

y_pred = rf_classifier.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
rf_acc = accuracy_score(y_test, y_pred)
rf_acc



"""### Overall Performance Visualization"""

plt.bar(['Logistic','KNN','SVM','kernel','Naive','Decision','Random'],[log_acc,knn_acc,svm_acc, kernel_acc,nb_acc,dt_acc, rf_acc])
plt.xlabel("Algorithms")
plt.ylabel("Accuracy")
plt.show()



"""## Save Model"""



from google.colab import drive
drive.mount('/content/drive')

import pickle
file = open('/content/drive/MyDrive/Data/stroke_model.pkl', 'wb')

pickle.dump(svm_classifier, file)